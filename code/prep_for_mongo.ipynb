{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "from collections import Counter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get freelancer profile by key\n",
    "\n",
    "Endpoint\n",
    "GET /api/profiles/v1/providers/{profile_key}.{format}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def json_prep(in_file):\n",
    "    # read the entire file into a python array\n",
    "    with open(in_file, 'rb') as f:\n",
    "        data = f.readlines()\n",
    "\n",
    "    # remove the trailing \"\\n\" from each line\n",
    "    data = map(lambda x: x.rstrip(), data)\n",
    "\n",
    "    # each element of 'data' is an individual JSON object.\n",
    "    # i want to convert it into an *array* of JSON objects\n",
    "    # which, in and of itself, is one large JSON object\n",
    "    # basically... add square brackets to the beginning\n",
    "    # and end, and have all the individual business JSON objects\n",
    "    # separated by a comma\n",
    "    data_json_str = \"[\" + ','.join(data) + \"]\"\n",
    "\n",
    "    data_list_of_dicts = json.loads(data_json_str)\n",
    "    \n",
    "#     now, load it into pandas\n",
    "#     out_df = pd.read_json(data_json_str)\n",
    "    return data_list_of_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make lists of dicts out of JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "as_profiles_lod = json_prep('../data/profiles_by_skill/apache-spark.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "655 <type 'list'>\n"
     ]
    }
   ],
   "source": [
    "print len(as_profiles_lod), type(as_profiles_lod)  # it's a list of dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ids = []\n",
    "for dict_item in as_profiles_lod:\n",
    "    ids.append(dict_item['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "655\n",
      "[u'~019fb185473d0c4789', u'~01db54f572948cbc45', u'~01b1e3d1a582370c49', u'~01a33ea6f9ce09ac24', u'~01a644f4e0579a50de']\n"
     ]
    }
   ],
   "source": [
    "print len(ids)\n",
    "print ids[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert into MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "# Access/Initiate Database\n",
    "db = client['upwork_db']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Access/Initiate Table\n",
    "profiles = db['apache-spark_profiles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x1174f7a00>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profiles.insert_many(as_profiles_lod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'apache-spark_profiles']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "655"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db[u'apache-spark_profiles'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'_id': ObjectId('571c223516d5977e5f226c39'),\n",
       " u'categories2': [u'IT & Networking'],\n",
       " u'country': u'Canada',\n",
       " u'description': u'10+ years experience as an Database (MySQL,Oracle,MS SQL) Administrator\\n\\n5+ years experience as an Linux, Ejabberd, Apache, Nginx,FreePBX,LVS, HAProxy administrator \\n\\n3+ years experience as an NoSQL(MongoDB,TokuMx), NewSQL(Cassandra, Impala),Data Warehouse (Infobright, Vertica), Elasticsearch, Logstash Administrator \\n\\nExcellent skills in implementing replication, allocate storage, upgrade, backup and recovery and maintain database access control and system security \\n\\nRecover corrupted table, dropped table without backup.\\n\\nWorked on databases of more than 10K QPS and data increased by TB each month. \\n\\nExpert in query tuning, schema and server optimization \\n\\nExperience in Percona toolkit, XtraDB cluster, different replication typologies with Tungsten replicator\\n\\nProven problem-solving ability in Linux administration and performance tuning\\n\\n\\nExtensive knowledge of database high availability design(master-slave, master-master,cluster), HAProxy, LVS\\n\\nFamily with Ejabber, Apache, Nginx, WordPress,PHP , Postfix,Sendmail configuration and optimization.',\n",
       " u'feedback': u'4.7926829268',\n",
       " u'id': u'~019fb185473d0c4789',\n",
       " u'last_activity': u'April 21, 2016',\n",
       " u'member_since': u'December 1, 2015',\n",
       " u'name': u'John Dolphin',\n",
       " u'portfolio_items_count': u'0',\n",
       " u'portrait_50': u'https://odesk-prod-portraits.s3.amazonaws.com/Users:johndolphin89:PortraitUrl_50?AWSAccessKeyId=1XVAX3FNQZAFC9GJCFR2&Expires=2147483647&Signature=7glp9KIR3bTKuWxeW9KbNDGgLBI%3D&1449075189788341',\n",
       " u'profile_type': u'Independent',\n",
       " u'rate': u'30.0',\n",
       " u'skills': [u'linux-system-administration',\n",
       "  u'database-adminstration',\n",
       "  u'mysql',\n",
       "  u'mongodb',\n",
       "  u'hadoop',\n",
       "  u'cassandra',\n",
       "  u'apache-spark',\n",
       "  u'apache-kafka',\n",
       "  u'performance-tuning',\n",
       "  u'clustering'],\n",
       " u'test_passed_count': u'4',\n",
       " u'title': u'Sr Database and System Administrator(LAMP)'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db[u'apache-spark_profiles'].find_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Deletes collection (table)\n",
    "db['test_table'].drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tab.find())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fl = ['ajax.txt', 'angularjs.txt', 'apache-hive.txt', 'apache-kafka.txt', 'apache-spark.txt', 'big-data.txt', 'bootstrap.txt', 'c#.txt', 'cassandra.txt', 'css.txt', 'css3.txt', 'data-analysis.txt', 'data-mining.txt', 'data-modeling.txt', 'data-science.txt', 'data-visualization.txt', 'django-framework.txt', 'hadoop.txt', 'hbase.txt', 'html.txt', 'html5.txt', 'ibm-spss.txt', 'java.txt', 'javascript.txt', 'jquery.txt', 'laravel-framework.txt', 'machine-learning.txt', 'mapreduce.txt', 'matlab.txt', 'mongodb.txt', 'mysql.txt', 'node.js.txt', 'nosql.txt', 'php.txt', 'pig.txt', 'postgresql.txt', 'predictive-analytics.txt', 'python-numpy.txt', 'python-scipy.txt', 'python.txt', 'r.txt', 'ruby-on-rails.txt', 'ruby.txt', 'sas.txt', 'scala.txt', 'spring-framework.txt', 'sql.txt', 'statistics.txt', 'tableau.txt', 'twitter-bootstrap.txt', 'wordpress.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', 'apache-hive.txt', 'apache-kafka.txt', 'apache-spark.txt', 'big-data.txt', 'cassandra.txt', 'data-analysis.txt', 'data-mining.txt', 'data-modeling.txt', 'data-science.txt', 'data-visualization.txt', 'full_stack', 'hadoop.txt', 'hbase.txt', 'ibm-spss.txt', 'java.txt', 'machine-learning.txt', 'mapreduce.txt', 'matlab.txt', 'mongodb.txt', 'mysql.txt', 'nosql.txt', 'pig.txt', 'predictive-analytics.txt', 'python-numpy.txt', 'python-scipy.txt', 'python.txt', 'r.txt', 'sas.txt', 'scala.txt', 'spring-framework.txt', 'sql.txt', 'statistics.txt', 'tableau.txt']\n"
     ]
    }
   ],
   "source": [
    "project_directory_path = '/Users/JerryTsai/userjst/individ/knowledg/cur/galvanize/project-upwork'\n",
    "# data_directory_path = os.getcwd() + '/data/profiles_by_skill'\n",
    "list_of_filenames = os.listdir(project_directory_path + '/data/profiles_by_skill')\n",
    "print list_of_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_ids_set = set()\n",
    "for filename in list_of_filenames:\n",
    "    # Set up \n",
    "    if filename[-4:] == '.txt':\n",
    "        skill_name = filename.split('.')[0].replace('-', '_').replace('#', 'Sharp')\n",
    "    filepath = project_directory_path + '/data/profiles_by_skill/' + filename\n",
    "    \n",
    "    profiles_lod = json_prep(filepath)\n",
    "    ids_list = []\n",
    "    for profile_as_dict in profiles_lod:\n",
    "        ids_list.append(profile_as_dict['id'])\n",
    "    \n",
    "    ids_set = set(ids_list)\n",
    "    print 'Skill: {}, ID count {}'.format(skill_name, len(ids_set))\n",
    "    all_ids_set.update(ids_set)\n",
    "    \n",
    "print 'All ID counts: {}'.format(len(all_ids_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do NOT run Below unless you make all_ids_set small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa;ii;jj;bb;cc\n",
      "dd;m;l;gg;ee\n",
      "hh;kk;ff;nn\n"
     ]
    }
   ],
   "source": [
    "list_size = 5\n",
    "counter = 0\n",
    "set_size_m_1 = len(all_ids_set) - 1\n",
    "for index, id in enumerate(all_ids_set):\n",
    "    remainder = counter % list_size\n",
    "    if remainder == 0:\n",
    "        id_list = []\n",
    "    id_list.append(id)\n",
    "    if remainder == (list_size-1) or counter == set_size_m_1:\n",
    "        profiles_str = ';'.join(id_list)\n",
    "        print profiles_str\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "The freelancer's profile key or a list of keys, separated by semicolon (';'). \n",
    "The number of keys per request is limited to 20."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
